---
title: 21 - 进程和线程
tags: 进程,线程,多线程,多进程,单线程,异步IO,协程,Process,multiprocessing,manager,BaseMananger,Queue,Pipes,local
renderNumberedHeading: true
grammar_cjkRuby: true
---

Author:  Qiao My
Create_Time: 2020-3-22

[toc]


# 〇、官档编程建议
- 避免共享状态
  - 应该尽可能避免在进程间传递大量数据，越少越好。
  - 最好坚持使用队列或者管道进行进程间通信，而不是底层的同步原语。
- 可序列化
  - 保证所代理的方法的参数是可以序列化的。
- 代理的线程安全性
  - 不要在多线程中同时使用一个代理对象，除非你用锁保护它。
- 使用 Join 避免僵尸进程
  - 在 Unix 上，如果一个进程执行完成但是没有被 join，就会变成僵尸进程。一般来说，僵尸进程不会很多，因为每次新启动进程（或者 active_children() 被调用）时，所有已执行完成且没有被 join 的进程都会自动被 join，而且对一个执行完的进程调用 Process.is_alive 也会 join 这个进程。
- 继承优于序列化、反序列化
  - 当使用 spawn 或者 forkserver 的启动方式时，multiprocessing 中的许多类型都必须是可序列化的，这样子进程才能使用它们。但是通常我们都应该避免使用管道和队列发送共享对象到另外一个进程，而是重新组织代码，对于其他进程创建出来的共享对象，让那些需要访问这些对象的子进程可以直接将这些对象从父进程继承过来。
- 避免杀死进程
  - 通过 Process.terminate 停止一个进程很容易导致这个进程正在使用的共享资源（如锁、信号量、管道和队列）损坏或者变得不可用，无法在其他进程中继续使用。
- Join 使用队列的进程
  - 记住，往队列放入数据的进程会一直等待直到队列中所有项被"feeder" 线程传给底层管道。（子进程可以调用队列的 Queue.cancel_join_thread 方法禁止这种行为）这意味着，任何使用队列的时候，你都要确保在进程join之前，所有存放到队列中的项将会被其他进程、线程完全消费。否则不能保证这个写过队列的进程可以正常终止。记住非精灵进程会自动 join 。
- 显示传递资源给子进程
  - 在Unix上，使用 fork 方式启动的子进程可以使用父进程中全局创建的共享资源。不过，最好是显式将资源对象通过参数的形式传递给子进程。除了（部分原因）让代码兼容 Windows 以及其他的进程启动方式外，这种形式还保证了在子进程生命期这个对象是不会被父进程垃圾回收的。如果父进程中的某些对象被垃圾回收会导致资源释放，这就变得很重要。
- 谨防将 sys.stdin 数据替换为 “类似文件的对象”




# 一、概念
进程：
操作系统中执行的一个程序，操作系统以进程为单位分配存储空间，每个进程都有自己的地址空间、数据栈以及其他用于跟踪进程执行的辅助数据，操作系统管理素有进程的执行，为它们合理的分配资源。进程可以通过 `fork` 或 `spawn` 的方式来创建新的进程来执行其他的任务，不过新的进程也有自己独立的内存空间，因此必须通过进程间通信机制(IPC，Inter-Process Communication)来实现数据共享，具体的方式包括**管道、信号、套接字、共享内存区**等。
线程：
进程的并发的执行线索，也就是获得CPU调度的执行单元。线程归属于进程，同一个进程内的多个线程可以共享上下文，因此相对于进程而言，线程间的信息共享和通信更加容易。当然在单核CPU系统中，真正的并发是不可能的，因为在某个时刻获得CPU的只有唯一的一个线程，多个线程共享了CPU的执行时间。
使用多线程实现并发编程为程序带来的好处是不言而喻的，最主要的体现在提升程序的性能和改善用户体验。

多线程的缺点在于，对其他进程并不友好，因为占用了更多的CPU执行时间，导致其他程序无法获得足够的CPU执行时间；另一方面，对开发者的要求也更高。

Python 既支持多进程也支持多线程。
Python实现并发主要有三种方式：多进程、多线程、多进程+多线程

# 二、Python 的 多进程
Unix 和Linux 上提供了`fork()`系统调用来创建进程，调用`fork()`函数时父进程，创建出的是子进程，子进程是父进程的一个拷贝，但是子进程拥有自己的PID。
`fork()`会返回想次，父进程中可以通过`fork()`函数的返回值得到子进程的PID，而子进程中的返回值永远都是0。子进程只需要调用`getppid()`就可以拿到父进程的ID。Python 的`os`模块提供了`fork()`函数。
由于Windows系统没有`fork()`调用，因此要实现跨平台的多进程变成，可以试用`multiprocessing`模块的`Process`类来创建子进程，而且该模块还提供了更高级的封装，例如批量启动进程的进程池(`Pool`)、用于进程间通信的队列(`Queue`)、管道(`Pipe`)


## 1. os.fork()
os模块提供的fork 方法，调用Unix、Linux系统的fork()方法实现，无法在Windows系统执行。
例1：
```python
import os

print('Process (%s) start...' % os.getpid())
# Only works on Unix/Linux/Mac:
pid = os.fork()
if pid == 0:
    print('I am child process (%s) and my parent is %s.' % (os.getpid(), os.getppid()))
else:
    print('I (%s) just created a child process (%s).' % (os.getpid(), pid))
# 输出
Process (876) start...
I (876) just created a child process (877).
I am child process (877) and my parent is 876.
```

## 2. multiprocessing 模块
跨平台版本的多进程模块
`multiprocessing`模块提供了一个`Process`类来代表一个进程对象。
创建子进程时，只需要传入执行函数和函数的参数，创建一个`Process`实例，用`start()`方法启动。
`join()`方法可以等待进程结束后再继续往下运行，通常用于进程间的同步。
例2：
```python
from multiprocessing import Process
import os

# 子进程要执行的代码
def run_proc(name):
    print('Run child process %s (%s)...' % (name, os.getpid()))

if __name__=='__main__':
    print('Parent process %s.' % os.getpid())
    p = Process(target=run_proc, args=('test',))
    print('Child process will start.')
    p.start()
    p.join()
    print('Child process end.')
# 执行结果
Parent process 928.
Child process will start.
Run child process test (929)...
Process end.
```

### Pool
启动大量子进程时，可以用进程池的方式批量创建子进程。
`Pool`对象调用`json()`的方法会等待所有子进程执行完毕，调用`json()`前必须先调用`close()`，调用了`close()`之后就不能继续添加新的`Process`了。
`Pool`的默认大小是CPU的核数。


例3：
```python
from multiprocessing import Pool
import os, time, random

def long_time_task(name):
    print('Run task %s (%s)...' % (name, os.getpid()))
    start = time.time()
    time.sleep(random.random() * 3)
    end = time.time()
    print('Task %s runs %0.2f seconds.' % (name, (end - start)))

if __name__=='__main__':
    print('Parent process %s.' % os.getpid())
    p = Pool(4)
    for i in range(5):
        p.apply_async(long_time_task, args=(i,))
    print('Waiting for all subprocesses done...')
    p.close()
    p.join()
    print('All subprocesses done.')
```

## 3. 子进程|subprocess 模块
很多时候，子进程并不是自身，而是一个外部进程。创建了子进程后，需要控制子进程的输入和输出。


### communicate()

例4：
```python
import subprocess

print('$ nslookup www.python.org')
r = subprocess.call(['nslookup', 'www.python.org'])
print('Exit code:', r)
# 等价于命令nslookup www.python.org，输出如下
$ nslookup www.python.org
Server:     192.168.19.4
Address:    192.168.19.4#53

Non-authoritative answer:
www.python.org  canonical name = python.map.fastly.net.
Name:   python.map.fastly.net
Address: 199.27.79.223

Exit code: 0
```

例5：
```python
import subprocess

print('$ nslookup')
p = subprocess.Popen(['nslookup'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
output, err = p.communicate(b'set q=mx\npython.org\nexit\n')
print(output.decode('utf-8'))
print('Exit code:', p.returncode)
# 等价于在命令行执行nslookup命令，然后手动输入set q=mx、python.org、exit，输出如下
$ nslookup
Server:     192.168.19.4
Address:    192.168.19.4#53

Non-authoritative answer:
python.org  mail exchanger = 50 mail.python.org.

Authoritative answers can be found from:
mail.python.org internet address = 82.94.164.166
mail.python.org has AAAA address 2001:888:2000:d::a6


Exit code: 0
```

## 4. 进程间通信
`POrcess`之间肯定需要通信的，操作系统提供了很多机制来实现进程间的通信。
Python 的 multiprocessing 模块包装了底层的机制，提供了`Queue`、`Pipes`等多种方式来交换数据。

由于Windows 没有`fork`调用，因此，`multiprocessing`需要“模拟”出`fork`的效果，父进程所有Python对象都必须通过 pickle 序列化再传到子进程去，所以，如果`multiprocessing`在Windows下调用失败了，要先考虑是不是pickle 失败了。

### Queue、Pipes


例6：
```python
from multiprocessing import Process, Queue
import os, time, random

# 写数据进程执行的代码:
def write(q):
    print('Process to write: %s' % os.getpid())
    for value in ['A', 'B', 'C']:
        print('Put %s to queue...' % value)
        q.put(value)
        time.sleep(random.random())

# 读数据进程执行的代码:
def read(q):
    print('Process to read: %s' % os.getpid())
    while True:
        value = q.get(True)
        print('Get %s from queue.' % value)

if __name__=='__main__':
    # 父进程创建Queue，并传给各个子进程：
    q = Queue()
    pw = Process(target=write, args=(q,))
    pr = Process(target=read, args=(q,))
    # 启动子进程pw，写入:
    pw.start()
    # 启动子进程pr，读取:
    pr.start()
    # 等待pw结束:
    pw.join()
    # pr进程里是死循环，无法等待其结束，只能强行终止:
    pr.terminate()
```



# 三、Python 的 多线程
Python 早期的版本中引入了 thread模块(现名为_thread) 来实现多线程编程，但该模块过于底层，而且很多功能都没有提供，因此，目前多线程开发推荐使用`threading`模块，该模块对多线程边恒提供了更好的面向对象的封装。


我们可以直接使用`threading`模块的`Thread`类来创建线程，也可以通过集成`Thread`类的方式来创建自定义的线程类，再通过创建线程对象并启动线程。


任何进程都会启动一个线程，该线程称为**主线程**，主线的名字叫做`MainThread`。
子进程的名字在创建时指定，仅仅在打印时用来显示，没有其他意义，默认命名为`Thread-n`。
`Threading`模块的`current_thread()`永远返回当前线程的实例。


## 1. threading 模块



### Lock
由于多个线程可以共享进程的内存空间，因此要实现多个线程间的通信相对简单，最直接的方法是设置一个全局变量，多个线程共享这个全局变量即可。
但当多个线程共享同一个变量(我们通常称之为“资源”)的时候，很可能产生不可控的结果从而导致程序失效甚至崩溃。如果一个资源被多个线程竞争使用，那么我们通常称之为**临界资源**，对“临界资源”的访问需要加上保护，否则资源会处于“混乱”的状态。

可以通过**锁**来保护临界资源，只有获得锁 的线程才能访问 临界资源，而其他没有得到锁 的线程只能被阻塞起来，直到获得锁 的线程释放了锁，其他线程才有机会获得锁 进而访问被保护的临界资源 。
创建一个锁，通过`threading.Lock()`来实现。
当多个线程同时执行`lock.acquire()`时，只有一个线程能成功获取锁，然后执行代码，其他线程需要继续等待直到获得锁为止。

锁的好处
- 确保某段关键代码只能由一个线程完整的执行
锁的坏处
- 阻止了多线程并发执行，包含锁的代码段实际只能单线程执行，效率大幅降低
- 由于可以存在多个锁，不同的线程持有不同的锁，并试图获取对方持有的锁时，可能造成死锁，导致多个线程全部挂起，只能靠操作系统强制终止。





>Python 的多线程并不能发挥CPU 的多核特性，因为Python解释器有一个**全局解释器锁Global Interpreter Lock (GIL)**的东西，任何线程执行前必须先获得 GIL 锁，然后每执行100条字节码，解释器就自动释放GIL 锁，让别的线程有机会执行，这是一个遗留问题。

# 四、ThreadLocal
`class threading.local`
一个代表线程本地数据的类。线程本地数据是特定线程的数据。管理线程本地数据，只需要创建一个 local （或者一个子类型）的实例并在实例中储存属性。

例7：
```python
import threading
    
# 创建全局ThreadLocal对象:
local_school = threading.local()

def process_student():
    # 获取当前线程关联的student:
    std = local_school.student
    print('Hello, %s (in %s)' % (std, threading.current_thread().name))

def process_thread(name):
    # 绑定ThreadLocal的student:
    local_school.student = name
    process_student()

t1 = threading.Thread(target= process_thread, args=('Alice',), name='Thread-A')
t2 = threading.Thread(target= process_thread, args=('Bob',), name='Thread-B')
t1.start()
t2.start()
t1.join()
t2.join()
# 输出
Hello, Alice (in Thread-A)
Hello, Bob (in Thread-B)
```
全局变量`local_school`就是一个`ThreadLocal`对象，每个`Thread`对它都可以读写`student`属性，但互不影响。你可以把`local_school`看成全局变量，但每个属性如`local_school.student`都是线程的局部变量，可以任意读写而互不干扰，也不用管理锁的问题，`ThreadLocal`内部会处理。

可以理解为全局变量`local_school`是一个`dict`，不但可以用`local_school.student`，还可以绑定其他变量，如`local_school.teacher`等等。

** `ThreadLocal`最常用的地方就是为每一个线程绑定一个数据库连接，HTTP请求，用户身份信息等，这样一个线程的所有调用到的处理函数都可以非常方便的访问这些资源。**

# 五、多进程还是多线程
无论多进程 还是 多线程，都需要进行切换。切换过程中，需要先保存当前执行的现场环境(CPU寄存器状态、内存页等)，然后，把新任务的执行环境准备好(恢复上次的寄存器状态、切换内存页等)，才能开始执行。这个过程虽然很快，但也要耗费时间。
当有几千个任务同时进行，操作系统很可能忙于切换任务，而没时间执行任务。所以，多任务一旦躲到一个限度，反而会使得系统性能急剧下降，最终导致所有任务都做不好。

CPU 密集型，如视频编码解码、格式转换等，主要靠CPU完成，适合单任务。多任务也可以完成，但会在切换任务上话费更多的CPU 时间。不过Python 脚本语言效率较低，可以通过嵌套 C/C++代码完成。

I/O 密集型，涉及到网络、存储介质的I/O，CPU 消耗很少，适用于多任务，可以减少I/O等待时间从而让CPU 高效率运转。



# 六、单线程+异步I/O
现代操作系统对I/O操作的改进中最为重要的就是支持异步I/O。如果充分利用操作系统支持的异步I/O支持，就可以用单进程单线程模型来执行多任务，这种模型称为 事件驱动模型。

在Python中，单线程+异步I/O 的变成模型称为`协程`.

协程的优势：
- 极高的执行效率，因为子程序切换不是线程切换，而是由程序自身控制，因此没有线程切换的开销。
- 不需要多线程的锁机制，因为只有一个线程，也不存在同时写变量冲突，在协程中控制共享资源不用加锁，只需要判断状态即可，这也带来更高的执行效率。

如果想要充分利用 CPU 的多核特性，最简单的方法时多进程+协程，既充分利用多核，又充分发挥协程的高效率，可获得极高的性能。



# 七、分布式进程
> 在Thread 和Process中，应该优选 Process，因为Process 更稳定，而且，Process可以分布到多台机器上，而Thread 最多只能分布到多个CPU上。


Python 的`multiprocessing`模块不但支持多进程，其中`managers`子模块还支持把多进程分布到多台机器上。
一个服务进程可以作为调度者，将任务分布到其他多个进程汇总，依靠网络通信。
由于`managers`模块封装很好，不必了解网络通信的细节，就可以很容易的编写分布式多进程程序。

例8：
机器1：服务进程。启动Queue，把Queue注册到网络上，然后向`Queue`写入任务。
```python
import random, time, queue
from multiprocessing.managers import BaseManager

# 发送任务的队列:
task_queue = queue.Queue()
# 接收结果的队列:
result_queue = queue.Queue()

# 从BaseManager继承的QueueManager:
class QueueManager(BaseManager):
    pass

# 把两个Queue都注册到网络上, callable参数关联了Queue对象:
QueueManager.register('get_task_queue', callable=lambda: task_queue)
QueueManager.register('get_result_queue', callable=lambda: result_queue)
# 绑定端口5000, 设置验证码'abc':
manager = QueueManager(address=('', 5000), authkey=b'abc')
# 启动Queue:
manager.start()
# 获得通过网络访问的Queue对象:
task = manager.get_task_queue()
result = manager.get_result_queue()
# 放几个任务进去:
for i in range(10):
    n = random.randint(0, 10000)
    print('Put task %d...' % n)
    task.put(n)
# 从result队列读取结果:
print('Try get results...')
for i in range(10):
    r = result.get(timeout=10)  # 等待10秒，这个过程中任务进程task_worker.py会对result队列进行写入
    print('Result: %s' % r)
# 关闭:
manager.shutdown()
print('master exit.')
```
机器2：任务进程
```python
# task_worker.py

import time, sys, queue
from multiprocessing.managers import BaseManager

# 创建类似的QueueManager:
class QueueManager(BaseManager):
    pass

# 由于这个QueueManager只从网络上获取Queue，所以注册时只提供名字:
QueueManager.register('get_task_queue')
QueueManager.register('get_result_queue')

# 连接到服务器，也就是运行task_master.py的机器:
server_addr = '127.0.0.1'
print('Connect to server %s...' % server_addr)
# 端口和验证码注意保持与task_master.py设置的完全一致:
m = QueueManager(address=(server_addr, 5000), authkey=b'abc')
# 从网络连接:
m.connect()
# 获取Queue的对象:
task = m.get_task_queue()
result = m.get_result_queue()
# 从task队列取任务,并把结果写入result队列:
for i in range(10):
    try:
        n = task.get(timeout=1)
        print('run task %d * %d...' % (n, n))
        r = '%d * %d = %d' % (n, n, n*n)
        time.sleep(1)
        result.put(r)
    except Queue.Empty:
        print('task queue is empty.')
# 处理结束:
print('worker exit.')
```

注意 ：
在一台机器上写多进程程序时，创建的`Queue`可直接拿来用，但在分布式多进程环境下，添加任务到`Queue`不可以直接对原始的`task_queue`进行操作，那样就绕过了`QueueManager`的封装，必须通过`manager.get_task_queue()`获得的`Queue`接口添加。



注意`task_worker.py` 中并没有创建`Queue`的代码，所以，`Queue`对象存储在`task_master.py`进程中。
如下图：
![multiprocessing.manager_queue_master_worker_廖雪峰](./images/multiprocessing.manager_queue_master_worker_廖雪峰.png)



`Queue`之所以能通过网络访问，就是通过`QueueManager`实现的。由于`QueueManager`管理的不止一个`Queue`，所以，要给每个`Queue`的网络调用接口起个名字，比如`get_task_queue`。
`authkey`用于保证两台机器正常通信，不被其他机器恶意干扰。


注意：
`Queue`的作用是用来传递任务和接收结果，每个任务的描述数量要尽量小。比如发送一个处理日志文件的任务，不要发送几百兆的日志文件本身，而是发送日志文件存放的完整路径，由`Worker`进程再去共享的磁盘上读取文件。























